{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93f4e32-66fd-4e9b-bd63-ed4a376a6e2a",
   "metadata": {},
   "source": [
    "# Pickrel-Smith_DSC680_Project3_Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dc0902-c1ac-4d80-b593-96f708e9d69e",
   "metadata": {},
   "source": [
    "## Main Script Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c794a124-293d-43ec-9853-b6533ceceb67",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd6f7c-63fa-4cc1-a10f-d9d5dcee232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import yfinance as yf\n",
    "import exchange_calendars as xcals\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e80a5-5427-4e6a-8ba0-93738e0aa7db",
   "metadata": {},
   "source": [
    "### Data Collection Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389bea3-af26-403d-8f79-fc01bac32ed4",
   "metadata": {},
   "source": [
    "#### Reddit Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e743c0-5084-4bc6-9610-da99a4aa57f8",
   "metadata": {},
   "source": [
    "##### fetch_reddit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af1757-774d-4620-a34d-ac52c5a02392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_reddit_data(subreddit, start_timestamp, end_timestamp, limit=100):\n",
    "    \"\"\"\n",
    "    Fetch posts from a subreddit within a specific time range using Pushshift API\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    subreddit : str\n",
    "        Name of the subreddit\n",
    "    start_timestamp : int\n",
    "        Unix timestamp for start date\n",
    "    end_timestamp : int\n",
    "        Unix timestamp for end date\n",
    "    limit : int\n",
    "        Maximum number of posts to retrieve per request\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of posts as dictionaries\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.pushshift.io/reddit/search/submission\"\n",
    "    \n",
    "    params = {\n",
    "        \"subreddit\": subreddit,\n",
    "        \"after\": start_timestamp,\n",
    "        \"before\": end_timestamp,\n",
    "        \"size\": limit,\n",
    "        \"sort\": \"asc\",\n",
    "        \"sort_type\": \"created_utc\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "        return data.get(\"data\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Reddit data: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d318b36-45c7-4908-829b-87ab64a6c6b0",
   "metadata": {},
   "source": [
    "##### fetch_reddit_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c71c7-df71-4579-bff1-72c714faca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_reddit_comments(submission_id, limit=100):\n",
    "    \"\"\"\n",
    "    Fetch comments for a specific Reddit submission using Pushshift API\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    submission_id : str\n",
    "        Reddit submission ID\n",
    "    limit : int\n",
    "        Maximum number of comments to retrieve\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of comments as dictionaries\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.pushshift.io/reddit/search/comment\"\n",
    "    \n",
    "    params = {\n",
    "        \"link_id\": submission_id,\n",
    "        \"size\": limit,\n",
    "        \"sort\": \"asc\",\n",
    "        \"sort_type\": \"created_utc\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "        return data.get(\"data\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Reddit comments: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32406932-72af-4746-8589-7931b3c1b337",
   "metadata": {},
   "source": [
    "##### save_to_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f6464-d5bf-4966-828b-3d53fa78f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet(data, filename):\n",
    "    \"\"\"\n",
    "    Save data to Parquet format\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : list or pandas.DataFrame\n",
    "        Data to save\n",
    "    filename : str\n",
    "        Output filename\n",
    "    \"\"\"\n",
    "    if isinstance(data, list):\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        df = data\n",
    "    \n",
    "    df.to_parquet(filename, index=False)\n",
    "    print(f\"Saved {len(df)} records to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ef4ca3-0e45-4af8-ba8d-a2de43d492eb",
   "metadata": {},
   "source": [
    "##### hash_usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d03c9-12e7-4234-b92c-654707db3ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_username(username):\n",
    "    \"\"\"\n",
    "    Hash a username to protect privacy\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    username : str\n",
    "        Username to hash\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Hashed username\n",
    "    \"\"\"\n",
    "    if pd.isna(username) or username is None:\n",
    "        return None\n",
    "    \n",
    "    return hashlib.sha256(str(username).encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb7fd6-5c4e-43a0-baf6-14c1c8afffdd",
   "metadata": {},
   "source": [
    "#### Stock Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c906b0d-f3cc-4f42-97b3-f66baa1c704a",
   "metadata": {},
   "source": [
    "##### fetch_stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0fb67e-4455-445b-955a-f81df8570315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch historical stock data from Yahoo Finance\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ticker : str\n",
    "        Stock ticker symbol\n",
    "    start_date : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    end_date : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing historical stock data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "        return stock_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching stock data: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c14464-a608-40b4-96ab-b55506d5c77c",
   "metadata": {},
   "source": [
    "##### get_trading_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a1880-88fe-4360-b0f3-8f3b32df2179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trading_days(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Get list of NYSE trading days between start_date and end_date\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    end_date : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DatetimeIndex\n",
    "        DatetimeIndex containing trading days\n",
    "    \"\"\"\n",
    "    nyse = xcals.get_calendar('NYSE')\n",
    "    trading_days = nyse.sessions_in_range(\n",
    "        pd.Timestamp(start_date),\n",
    "        pd.Timestamp(end_date)\n",
    "    )\n",
    "    return trading_days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1601d-9aad-4f1c-841f-5956e1da07a8",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62fefac-85e3-4c14-932c-ff620ccf50f6",
   "metadata": {},
   "source": [
    "##### analyze_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f195e3-6578-49d1-8ec2-345b498cb4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Analyze sentiment of text using VADER\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Text to analyze\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing sentiment scores\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return {\n",
    "            'compound': 0,\n",
    "            'pos': 0,\n",
    "            'neu': 0,\n",
    "            'neg': 0\n",
    "        }\n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return analyzer.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373cc51-ffa7-45dd-bf11-2659ce32b684",
   "metadata": {},
   "source": [
    "##### process_reddit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0fbc7-fe6a-4202-a50e-558507f9b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reddit_data(df):\n",
    "    \"\"\"\n",
    "    Process Reddit data: clean and add sentiment scores\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing Reddit posts or comments\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Processed DataFrame with sentiment scores\n",
    "    \"\"\"\n",
    "    # Convert timestamps to datetime\n",
    "    if 'created_utc' in df.columns:\n",
    "        df['created_at'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "    \n",
    "    # Hash usernames for privacy\n",
    "    if 'author' in df.columns:\n",
    "        df['author_hashed'] = df['author'].apply(hash_username)\n",
    "    \n",
    "    # Analyze sentiment for titles and text/body\n",
    "    sentiment_columns = []\n",
    "    \n",
    "    if 'title' in df.columns:\n",
    "        df['title_sentiment'] = df['title'].apply(analyze_sentiment)\n",
    "        df['title_compound'] = df['title_sentiment'].apply(lambda x: x['compound'])\n",
    "        sentiment_columns.append('title_compound')\n",
    "    \n",
    "    text_col = next((col for col in ['selftext', 'body'] if col in df.columns), None)\n",
    "    if text_col:\n",
    "        df[f'{text_col}_sentiment'] = df[text_col].apply(analyze_sentiment)\n",
    "        df[f'{text_col}_compound'] = df[f'{text_col}_sentiment'].apply(lambda x: x['compound'])\n",
    "        sentiment_columns.append(f'{text_col}_compound')\n",
    "    \n",
    "    # Calculate overall sentiment if multiple sentiment columns exist\n",
    "    if len(sentiment_columns) > 0:\n",
    "        df['compound_sentiment'] = df[sentiment_columns].mean(axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc15422-8cbf-4762-83cb-e3b4618564f6",
   "metadata": {},
   "source": [
    "##### aggregate_daily_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f0825-41f7-417b-b18c-546cde0ec640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_daily_sentiment(df, date_column='created_at'):\n",
    "    \"\"\"\n",
    "    Aggregate sentiment scores by day\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing sentiment scores\n",
    "    date_column : str\n",
    "        Column containing dates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with daily aggregated sentiment\n",
    "    \"\"\"\n",
    "    # Ensure date column is datetime\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    \n",
    "    # Group by date and calculate statistics\n",
    "    daily_sentiment = df.groupby(df[date_column].dt.date).agg({\n",
    "        'compound_sentiment': ['mean', 'median', 'std', 'count'],\n",
    "        'title_compound': ['mean', 'median', 'std'] if 'title_compound' in df.columns else [],\n",
    "        'selftext_compound': ['mean', 'median', 'std'] if 'selftext_compound' in df.columns else [],\n",
    "        'body_compound': ['mean', 'median', 'std'] if 'body_compound' in df.columns else []\n",
    "    })\n",
    "    \n",
    "    # Flatten multi-index columns\n",
    "    daily_sentiment.columns = ['_'.join(col).strip() for col in daily_sentiment.columns.values]\n",
    "    \n",
    "    # Reset index to make date a column\n",
    "    daily_sentiment = daily_sentiment.reset_index()\n",
    "    daily_sentiment.rename(columns={date_column: 'date'}, inplace=True)\n",
    "    \n",
    "    return daily_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c00f12-cb94-4a00-8a32-919999311056",
   "metadata": {},
   "source": [
    "### Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d79688-aa95-4bda-9e87-b33966965c11",
   "metadata": {},
   "source": [
    "##### prepare_stock_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700e5c4-74cd-488b-a23f-f6604f797f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_stock_features(stock_data):\n",
    "    \"\"\"\n",
    "    Prepare stock features for modeling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stock_data : pandas.DataFrame\n",
    "        DataFrame containing stock data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    stock_data['daily_return'] = stock_data['Adj Close'].pct_change()\n",
    "    \n",
    "    # Calculate 5-day moving average\n",
    "    stock_data['ma5'] = stock_data['Adj Close'].rolling(window=5).mean()\n",
    "    \n",
    "    # Create moving average crossover flag\n",
    "    stock_data['ma_crossover'] = (stock_data['Adj Close'] > stock_data['ma5']).astype(int)\n",
    "    \n",
    "    # Create target variable: price direction (up, down, flat)\n",
    "    # Define 'flat' as daily return between -0.2% and 0.2%\n",
    "    def categorize_return(ret):\n",
    "        if pd.isna(ret):\n",
    "            return None\n",
    "        elif ret > 0.002:\n",
    "            return 'up'\n",
    "        elif ret < -0.002:\n",
    "            return 'down'\n",
    "        else:\n",
    "            return 'flat'\n",
    "    \n",
    "    stock_data['price_direction'] = stock_data['daily_return'].apply(categorize_return)\n",
    "    \n",
    "    # Create lagged features\n",
    "    stock_data['prev_return'] = stock_data['daily_return'].shift(1)\n",
    "    stock_data['prev_direction'] = stock_data['price_direction'].shift(1)\n",
    "    \n",
    "    return stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dddcddf-4e91-45f7-9955-eab00a3a2e62",
   "metadata": {},
   "source": [
    "##### merge_sentiment_stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388884f0-d823-436c-a1d7-725ac21ab93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentiment_stock_data(sentiment_data, stock_data, trading_days):\n",
    "    \"\"\"\n",
    "    Merge sentiment data with stock data, aligning by trading days\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sentiment_data : pandas.DataFrame\n",
    "        DataFrame containing sentiment data\n",
    "    stock_data : pandas.DataFrame\n",
    "        DataFrame containing stock data\n",
    "    trading_days : pandas.DatetimeIndex\n",
    "        DatetimeIndex containing trading days\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Merged DataFrame\n",
    "    \"\"\"\n",
    "    # Convert date columns to datetime\n",
    "    sentiment_data['date'] = pd.to_datetime(sentiment_data['date'])\n",
    "    \n",
    "    # Create a mapping from calendar days to next trading days\n",
    "    calendar_to_trading = {}\n",
    "    trading_days_list = sorted(trading_days.date)\n",
    "    \n",
    "    for i, day in enumerate(trading_days_list[:-1]):\n",
    "        # Map each calendar day to the next trading day\n",
    "        calendar_to_trading[day] = trading_days_list[i+1]\n",
    "    \n",
    "    # Add a column with the next trading day\n",
    "    sentiment_data['next_trading_day'] = sentiment_data['date'].apply(\n",
    "        lambda x: calendar_to_trading.get(x.date(), None)\n",
    "    )\n",
    "    \n",
    "    # Drop rows where next_trading_day is None\n",
    "    sentiment_data = sentiment_data.dropna(subset=['next_trading_day'])\n",
    "    \n",
    "    # Convert next_trading_day to datetime for merging\n",
    "    sentiment_data['next_trading_day'] = pd.to_datetime(sentiment_data['next_trading_day'])\n",
    "    \n",
    "    # Reset index of stock_data to make Date a column\n",
    "    stock_data = stock_data.reset_index()\n",
    "    \n",
    "    # Merge sentiment data with stock data\n",
    "    merged_data = pd.merge(\n",
    "        sentiment_data,\n",
    "        stock_data,\n",
    "        left_on='next_trading_day',\n",
    "        right_on='Date',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc193d-48d4-4c4f-8b6a-7123586cf3e2",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf5046e-70a2-4281-ab9a-b076524ca91f",
   "metadata": {},
   "source": [
    "##### train_evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9aea93-03ae-4c5a-a04f-7afdd26a6886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(data, features, target, n_splits=5):\n",
    "    \"\"\"\n",
    "    Train and evaluate a multinomial logistic regression model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        DataFrame containing features and target\n",
    "    features : list\n",
    "        List of feature column names\n",
    "    target : str\n",
    "        Target column name\n",
    "    n_splits : int\n",
    "        Number of splits for time series cross-validation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (model, evaluation metrics)\n",
    "    \"\"\"\n",
    "    # Drop rows with missing values\n",
    "    data = data.dropna(subset=features + [target])\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = data[features]\n",
    "    y = data[target]\n",
    "    \n",
    "    # Handle class imbalance\n",
    "    class_weights = {\n",
    "        'up': len(y) / (3 * (y == 'up').sum()),\n",
    "        'down': len(y) / (3 * (y == 'down').sum()),\n",
    "        'flat': len(y) / (3 * (y == 'flat').sum())\n",
    "    }\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        solver='lbfgs',\n",
    "        class_weight=class_weights,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Perform time series cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    cv_results = {\n",
    "        'accuracy': [],\n",
    "        'balanced_accuracy': [],\n",
    "        'macro_f1': []\n",
    "    }\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        cv_results['accuracy'].append(report['accuracy'])\n",
    "        cv_results['balanced_accuracy'].append(balanced_accuracy_score(y_test, y_pred))\n",
    "        cv_results['macro_f1'].append(report['macro avg']['f1-score'])\n",
    "    \n",
    "    # Train final model on all data\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean(cv_results['accuracy']),\n",
    "        'balanced_accuracy': np.mean(cv_results['balanced_accuracy']),\n",
    "        'macro_f1': np.mean(cv_results['macro_f1'])\n",
    "    }\n",
    "    \n",
    "    return model, avg_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2cb1b4-30fc-47a0-ac61-39364278ac93",
   "metadata": {},
   "source": [
    "##### compare_with_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82acc478-3266-4a69-8ca8-959616d81b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_baseline(data, target):\n",
    "    \"\"\"\n",
    "    Compare model performance with a naive baseline\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        DataFrame containing the target\n",
    "    target : str\n",
    "        Target column name\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Baseline metrics\n",
    "    \"\"\"\n",
    "    # Naive baseline: always predict 'flat'\n",
    "    y_true = data[target].dropna()\n",
    "    y_pred = pd.Series(['flat'] * len(y_true), index=y_true.index)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    \n",
    "    baseline_metrics = {\n",
    "        'accuracy': report['accuracy'],\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        'macro_f1': report['macro avg']['f1-score']\n",
    "    }\n",
    "    \n",
    "    return baseline_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18dee55-19b9-49ce-a848-cb64bac5e92b",
   "metadata": {},
   "source": [
    "### Main Execution Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5455026a-6600-4423-a1e1-abb902ffeb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "subreddits = ['AMD', 'StockMarket']\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2025-05-01'  # Current date\n",
    "ticker = 'AMD'\n",
    "data_dir = 'data'\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Convert dates to timestamps\n",
    "start_timestamp = int(datetime.datetime.strptime(start_date, '%Y-%m-%d').timestamp())\n",
    "end_timestamp = int(datetime.datetime.strptime(end_date, '%Y-%m-%d').timestamp())\n",
    "\n",
    "# Fetch Reddit data for each subreddit\n",
    "for subreddit in subreddits:\n",
    "    print(f\"Fetching posts from r/{subreddit}...\")\n",
    "    \n",
    "    # Initialize an empty list to store all posts\n",
    "    all_posts = []\n",
    "    all_comments = []\n",
    "    \n",
    "    # Fetch posts in chunks\n",
    "    current_timestamp = start_timestamp\n",
    "    \n",
    "    while current_timestamp < end_timestamp:\n",
    "        # Calculate end of chunk (30 days later)\n",
    "        chunk_end = min(current_timestamp + 30*24*60*60, end_timestamp)\n",
    "        \n",
    "        # Fetch posts for this chunk\n",
    "        posts = fetch_reddit_data(subreddit, current_timestamp, chunk_end)\n",
    "        \n",
    "        if posts:\n",
    "            all_posts.extend(posts)\n",
    "            \n",
    "            # Fetch comments for each post\n",
    "            for post in tqdm(posts, desc=f\"Fetching comments for r/{subreddit} posts\"):\n",
    "                if 'id' in post:\n",
    "                    comments = fetch_reddit_comments(f\"t3_{post['id']}\")\n",
    "                    all_comments.extend(comments)\n",
    "            \n",
    "            # Save intermediate results\n",
    "            save_to_parquet(all_posts, f\"{data_dir}/{subreddit}_posts.parquet\")\n",
    "            save_to_parquet(all_comments, f\"{data_dir}/{subreddit}_comments.parquet\")\n",
    "        \n",
    "        # Move to next chunk\n",
    "        current_timestamp = chunk_end\n",
    "        \n",
    "        # Sleep to avoid hitting API rate limits\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"Collected {len(all_posts)} posts and {len(all_comments)} comments from r/{subreddit}\")\n",
    "\n",
    "# Fetch stock data\n",
    "print(f\"Fetching stock data for {ticker}...\")\n",
    "stock_data = fetch_stock_data(ticker, start_date, end_date)\n",
    "stock_data.to_parquet(f\"{data_dir}/{ticker}_stock_data.parquet\")\n",
    "\n",
    "# Get trading days\n",
    "trading_days = get_trading_days(start_date, end_date)\n",
    "\n",
    "# Process Reddit data\n",
    "all_sentiment_data = []\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    # Load posts\n",
    "    posts_df = pd.read_parquet(f\"{data_dir}/{subreddit}_posts.parquet\")\n",
    "    \n",
    "    # Process posts\n",
    "    processed_posts = process_reddit_data(posts_df)\n",
    "    \n",
    "    # Load comments\n",
    "    comments_df = pd.read_parquet(f\"{data_dir}/{subreddit}_comments.parquet\")\n",
    "    \n",
    "    # Process comments\n",
    "    processed_comments = process_reddit_data(comments_df)\n",
    "    \n",
    "    # Aggregate daily sentiment\n",
    "    daily_posts_sentiment = aggregate_daily_sentiment(processed_posts)\n",
    "    daily_comments_sentiment = aggregate_daily_sentiment(processed_comments)\n",
    "    \n",
    "    # Add subreddit column\n",
    "    daily_posts_sentiment['subreddit'] = subreddit\n",
    "    daily_comments_sentiment['subreddit'] = subreddit\n",
    "    \n",
    "    # Add data type column\n",
    "    daily_posts_sentiment['data_type'] = 'posts'\n",
    "    daily_comments_sentiment['data_type'] = 'comments'\n",
    "    \n",
    "    # Append to all sentiment data\n",
    "    all_sentiment_data.append(daily_posts_sentiment)\n",
    "    all_sentiment_data.append(daily_comments_sentiment)\n",
    "\n",
    "# Combine all sentiment data\n",
    "combined_sentiment = pd.concat(all_sentiment_data, ignore_index=True)\n",
    "\n",
    "# Save combined sentiment data\n",
    "combined_sentiment.to_parquet(f\"{data_dir}/combined_sentiment.parquet\")\n",
    "\n",
    "# Prepare stock features\n",
    "stock_data = prepare_stock_features(stock_data)\n",
    "\n",
    "# Merge sentiment with stock data\n",
    "merged_data = merge_sentiment_stock_data(combined_sentiment, stock_data, trading_days)\n",
    "\n",
    "# Save merged data\n",
    "merged_data.to_csv(f\"{data_dir}/merged_data.csv\", index=False)\n",
    "\n",
    "# Define features for modeling\n",
    "sentiment_features = [\n",
    "    'compound_sentiment_mean', \n",
    "    'compound_sentiment_std',\n",
    "    'compound_sentiment_count'\n",
    "]\n",
    "\n",
    "stock_features = [\n",
    "    'prev_return',\n",
    "    'ma_crossover'\n",
    "]\n",
    "\n",
    "all_features = sentiment_features + stock_features\n",
    "target = 'price_direction'\n",
    "\n",
    "# Train and evaluate model\n",
    "model, model_metrics = train_evaluate_model(merged_data, all_features, target)\n",
    "\n",
    "# Compare with baseline\n",
    "baseline_metrics = compare_with_baseline(merged_data, target)\n",
    "\n",
    "# Print results\n",
    "print(\"Model Performance:\")\n",
    "for metric, value in model_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nBaseline Performance:\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Save model\n",
    "import pickle\n",
    "with open(f\"{data_dir}/sentiment_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(f\"Model saved to {data_dir}/sentiment_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4652c0cb-fa27-4c9b-baf0-2621493f3dd6",
   "metadata": {},
   "source": [
    "### Streamlit Dashboard Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a5e3f-c7c7-400e-a023-2881e56e80cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1216f595-f047-4f31-a500-31a8ce3c368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    with open(\"data/sentiment_model.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "# Load data\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    return pd.read_csv(\"data/merged_data.csv\")\n",
    "\n",
    "# Fetch recent sentiment data\n",
    "@st.cache_data\n",
    "def get_recent_sentiment():\n",
    "    # In a real application, this would fetch new data from Reddit\n",
    "    # For this demo, we'll use the most recent data from our dataset\n",
    "    data = load_data()\n",
    "    return data.sort_values('date', ascending=False).head(7)\n",
    "\n",
    "# Fetch recent stock data\n",
    "@st.cache_data\n",
    "def get_recent_stock_data():\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=30)\n",
    "    return yf.download('AMD', start=start_date, end=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea831bb-b7b6-4b9c-8dbe-8d4c7ddaf6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "def predict_next_day(model, sentiment_data, stock_data):\n",
    "    # Prepare features\n",
    "    features = {\n",
    "        'compound_sentiment_mean': sentiment_data['compound_sentiment_mean'].iloc[0],\n",
    "        'compound_sentiment_std': sentiment_data['compound_sentiment_std'].iloc[0],\n",
    "        'compound_sentiment_count': sentiment_data['compound_sentiment_count'].iloc[0],\n",
    "        'prev_return': stock_data['Adj Close'].pct_change().iloc[-1],\n",
    "        'ma_crossover': (stock_data['Adj Close'].iloc[-1] > \n",
    "                         stock_data['Adj Close'].rolling(window=5).mean().iloc[-1]).astype(int)\n",
    "    }\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    features_df = pd.DataFrame([features])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(features_df)[0]\n",
    "    probabilities = model.predict_proba(features_df)[0]\n",
    "    \n",
    "    return prediction, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4452a-9198-4279-8174-4b2e9696c898",
   "metadata": {},
   "source": [
    "### Dashboard Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acecd48-9618-4ea9-a066-80e65dfa7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the dashboard title\n",
    "st.title(\"Reddit Sentiment & AMD Stock Prediction\")\n",
    "\n",
    "# Load data and model\n",
    "model = load_model()\n",
    "data = load_data()\n",
    "recent_sentiment = get_recent_sentiment()\n",
    "recent_stock = get_recent_stock_data()\n",
    "\n",
    "# Make prediction\n",
    "prediction, probabilities = predict_next_day(model, recent_sentiment, recent_stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fc1af-15e2-4977-bd0e-45ecc6b53ba0",
   "metadata": {},
   "source": [
    "### Prediction Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa7776-9f9e-4346-9a67-192147af0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display prediction\n",
    "st.header(\"Next Day Prediction\")\n",
    "\n",
    "col1, col2, col3 = st.columns(3)\n",
    "\n",
    "with col1:\n",
    "    st.metric(\"Prediction\", prediction.upper())\n",
    "\n",
    "with col2:\n",
    "    st.metric(\"Current Price\", f\"${recent_stock['Adj Close'].iloc[-1]:.2f}\")\n",
    "\n",
    "with col3:\n",
    "    prev_day_return = recent_stock['Adj Close'].pct_change().iloc[-1] * 100\n",
    "    st.metric(\"Previous Day Return\", f\"{prev_day_return:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a0570-7210-4a19-89ea-ad1c743d4540",
   "metadata": {},
   "source": [
    "### Prediction Probability Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b05c7b-7ebe-4446-801d-46eb823af442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display probability chart\n",
    "st.subheader(\"Prediction Probabilities\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "labels = model.classes_\n",
    "probs = probabilities * 100\n",
    "\n",
    "ax.bar(labels, probs, color=['red', 'gray', 'green'])\n",
    "ax.set_ylabel('Probability (%)')\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "for i, v in enumerate(probs):\n",
    "    ax.text(i, v + 1, f\"{v:.1f}%\", ha='center')\n",
    "\n",
    "st.pyplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461d90a-500d-4d88-bd80-1d2b14992822",
   "metadata": {},
   "source": [
    "### Sentiment Trend Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40920496-5178-46f1-b67b-b5a433603d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display recent sentiment\n",
    "st.header(\"Recent Reddit Sentiment\")\n",
    "\n",
    "# Plot sentiment over time\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "sentiment_plot_data = recent_sentiment.sort_values('date')\n",
    "ax.plot(sentiment_plot_data['date'], sentiment_plot_data['compound_sentiment_mean'], \n",
    "        marker='o', linestyle='-', label='Mean Sentiment')\n",
    "\n",
    "ax.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "ax.set_ylabel('Compound Sentiment Score')\n",
    "ax.set_title('Reddit Sentiment (Last 7 Days)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "st.pyplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3814c643-b046-469c-8752-93763708dae0",
   "metadata": {},
   "source": [
    "### Stock Price Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab02023-d032-44ea-84f4-fe3e141c2641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display recent stock price\n",
    "st.header(\"Recent AMD Stock Price\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(recent_stock.index, recent_stock['Adj Close'], label='AMD')\n",
    "ax.set_ylabel('Price ($)')\n",
    "ax.set_title('AMD Stock Price (Last 30 Days)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "st.pyplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6009591d-24f1-4199-aaba-7457592c6f6d",
   "metadata": {},
   "source": [
    "### Raw Data Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05916218-22b2-432d-b6d1-ca6cbf175938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display raw data\n",
    "with st.expander(\"View Raw Sentiment Data\"):\n",
    "    st.dataframe(recent_sentiment)\n",
    "\n",
    "with st.expander(\"View Raw Stock Data\"):\n",
    "    st.dataframe(recent_stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3218be-b7f1-4395-9332-70f1d3bba02b",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This project has built a lightweight, reproducible pipeline that:\n",
    "1. Collects Reddit posts and comments from r/AMD and r/StockMarket\n",
    "2. Analyzes sentiment using the VADER lexicon\n",
    "3. Merges sentiment data with AMD stock prices\n",
    "4. Trains a multinomial logistic regression model to predict price direction\n",
    "5. Creates a Streamlit dashboard for visualization\n",
    "\n",
    "The model achieves [metrics] performance, which is [comparison] to the naive baseline.\n",
    "\n",
    "### Limitations\n",
    "- VADER may not accurately capture sarcasm and slang\n",
    "- Reddit API availability and rate limits\n",
    "- Class imbalance in stock price movements\n",
    "- Short-term market movements are inherently noisy\n",
    "\n",
    "### Next Steps\n",
    "- Deploy the Streamlit dashboard\n",
    "- Implement daily data collection for real-time predictions\n",
    "- Explore more advanced NLP techniques\n",
    "- Add more technical indicators as features\n",
    "- Test the approach on other stocks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
